---
layout: post
title: Fully walk through the Chipmunk, a Training-Free Acceleration method for Diffusion Transformers
---

In my very first post here in my personal blog, I'd like to give a fully walk through of the [Chipmunk: Training-Free Acceleration of Diffusion Transformers with Dynamic Column-Sparse Deltas](https://arxiv.org/abs/2506.03275) paper. This paper utilize the well-known property of the diffusion models when generating images and videos, that is, the adjacent steps in the diffusion process are somewhat correlated. Authors use this property to accelerate the inference of diffusion transformers by using a sparse matrix to calculate the delta between the adjacent steps. Futhermore, they also implement the kernel for their algorithm to make it more efficient and performant. The results shows very promising results, for example, it can accelerate the inference of FLUX.1-dev by ~1.6x when generating 1280x768 images in 50 steps.

Although authors from Sandy Research in UCSD already wrote [a series of blog posts](https://sandyresearch.github.io/) and also had a [youtube video](https://youtu.be/Rg9enIRSXmo?si=aB7ZcoG5xfuTlip8) to introduce the algorithm, I still want to write this post to give a fully walk through of the algorithm and the codebase to help myself learning by rewriting in my own words.

In this post, I'll start with the algorithm and paper itself first, and then jump into the codebase that [open-sourced by the authors](https://github.com/sandyresearch/chipmunk). I'll also provide steps that I reproduce the results by the codebase in the end. If you are not interested in the technical details, and just want to adopt the algorithm to your own projects, you can skip to the [Running Chipmunk on diffusion models directly](#running-chipmunk-on-diffusion-models-directly) section. Without further ado, let's get started!

## Introduction

<!---Diffusion model explosion in image/video generation-->
Starting from the very first appearance of the diffusion models that used to genearte images from noise in orign paper [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239), the diffusion models have been widely used in the field of image/video generation. Both architetures and algorithms have been evolving from then on. For the architectures, original paper uses a U-Net to denoise in the pixel space until the intro of [LDM](https://arxiv.org/abs/2112.10752) which uses an autoencoder to embed the image into the latent space. The U-Net is then replaced by a [transformer-based architecture called DiT](https://arxiv.org/abs/2212.09748) in the later works for higher performance, i.e. lower FID score. From the algorithm side, a lot of methods have been proposed to either improve the performance of the diffusion process or to accelerate the inference of the diffusion process. The forward process remains stochastic to model noise corruption, but the reverse process has evolved from stochastic sampling (SDE) to [deterministic trajectories (ODE) in DDIM](https://arxiv.org/abs/2010.02502), and even direct [flow-based mappings in recent works](https://arxiv.org/abs/2210.02747). Most of the newest generative models including [FLUX.1 Kontext](https://arxiv.org/abs/2506.15742) for image geneneration, [MovieGen](https://arxiv.org/abs/2410.13720) and [Wan2.1](https://arxiv.org/abs/2503.20314).


<!---Computation cost and efficiency-->






## Method

## Experiments

## Conclusion